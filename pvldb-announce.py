#!/usr/bin/env python

import os
import sys
import re
import urllib
import logging
import pytz
import argparse
import sqlite3
import twitter
from datetime import datetime
from datetime import tzinfo
from pprint import pprint
from feedgen.feed import FeedGenerator
from bs4 import BeautifulSoup

## ==============================================
## LOGGING
## ==============================================
LOG = logging.getLogger(__name__)
LOG_handler = logging.StreamHandler()
LOG_formatter = logging.Formatter(fmt='%(asctime)s [%(funcName)s:%(lineno)03d] %(levelname)-5s: %(message)s',
                                  datefmt='%m-%d-%Y %H:%M:%S')
LOG_handler.setFormatter(LOG_formatter)
LOG.addHandler(LOG_handler)
LOG.setLevel(logging.INFO)

## ==============================================
## CONFIGURATION
## ==============================================

RSS_TITLE = 'PVLDB Paper Announcements'
RSS_AUTHOR = {'name':'Andy Pavlo','email':'pavlo@cs.cmu.edu'}
RSS_SUBTITLE = 'Generated by the Carnegie Mellon Database Group'
RSS_FILE = "pvldb-rss.xml"
RSS_URL = "http://db.cs.cmu.edu/files/" + RSS_FILE
BASE_URL = "http://www.vldb.org/pvldb/"

DB_PATH = os.path.join(os.path.dirname(os.path.realpath(__file__)), "pvldb.db")

dateFormat = "%B %Y"
dateRe = re.compile("Volume ([\d]+), No\. ([\d]+), ([A-Z][a-z]+ [\d]{4})")

SKIP = set([ "vol%d.html" % x for x in xrange(1, 5) ])

VOLUME_LABELS = { }

## ==============================================
## getVolumeUrls
## ==============================================
def getVolumeUrls(url):
    volumes = [ ]
    r = urllib.urlopen(url).read()
    soup = BeautifulSoup(r, "lxml")
    regex = re.compile("vol[\d]+\.html")
    for a in soup.find_all('a'):
        m = regex.match(a["href"])
        if m and not a["href"] in volumes:
            if a["href"] in SKIP:
                logging.warn("Skipping '%s'" % a["href"])
                continue
            volumes.append(a["href"])
    ## FOR
    return [ BASE_URL + x for x in volumes ]
## DEF

## ==============================================
## getPapers
## ==============================================
def getPapers(vol_url):
    r = urllib.urlopen(vol_url).read()
    soup = BeautifulSoup(r, "lxml")
    
    papers = { }
    for s in soup.find_all('h2'):
        sectionDate = None
        m = dateRe.match(s.text)
        if m:
            sectionDate = datetime.strptime(m.groups()[2], dateFormat)
            volume = int(m.groups()[0])
            number = int(m.groups()[1])
            VOLUME_LABELS[sectionDate] = (volume, number)
        assert not sectionDate is None
        
        if not sectionDate in papers:
            papers[sectionDate] = [ ]
        
        ul_search = s.find_next('ul')
        # VOL6 Fix
        if vol_url.find("vol6.html") != -1:
            ul_search = ul_search.find_next('ul')
            
        for u in ul_search:
            skip = False
            try:
                for p in u.parent.find_all('li'):
                    if p.text.find("Editors-in-Chief") != -1:
                        skip = True
                    if skip: continue
                        
                    url = None
                    title = None
                    authors = None
                    
                    link = p.find("a")
                    if link and not p.next_element is None:
                        if link.text.find("Front Matter") != -1:
                            continue
                        authors = p.next_element
                        try:
                            authors = authors.strip()[:-1]
                        except:
                            logging.error("authors=" + str(type(authors)))
                            logging.error("authors.text=" + str(authors.text))
                            raise
                        url = link["href"]
                        if not url.startswith("http://"):
                            url = BASE_URL + url
                        title = link.__dict__["contents"][0].replace("\n", " ").strip()
                        
                        #print pprint(dir(link))
                        #print
                    #print
                    if url is None: continue
                    papers[sectionDate].append({
                        "authors":      authors,
                        "title":        title,
                        "volume":       volume,
                        "number":       number,
                        "link":         url,
                        "published":    sectionDate.replace(tzinfo=pytz.utc)
                    })
                ## FOR
                break
            except:
                logging.error("Unexpected error for section '" + s.text + "'")
                logging.error("link=" + str(link))
                logging.error("p=" + str(p))
                #logging.error("authors=" + str(authors))
                raise
        ## FOR
    ## FOR
    return (papers)
## DEF

## ==============================================
## writeRSS
## ==============================================
def writeRSS(papers, output):
    fg = FeedGenerator()
    fg.id(RSS_URL)
    fg.title(RSS_TITLE)
    fg.subtitle(RSS_SUBTITLE)
    fg.author(RSS_AUTHOR)
    fg.link( href='http://www.vldb.org/pvldb/', rel='alternate' )
    fg.language('en')
    
    for d in reversed(sorted(papers.keys())):
        date = d.replace(tzinfo=pytz.utc)
        volume = VOLUME_LABELS[d][0]
        number = VOLUME_LABELS[d][1]
        for p in papers[d]:
            summary = "%s\nAuthors: %s\nPVLDB Volume %d, Number %d" % (p[1], p[0], volume, number)
            
            fe = fg.add_entry()
            fe.author(name=p[0])
            fe.title(p[1])
            fe.link(href=p[2]) 
            fe.id(p[2])
            fe.published(published=date)
            fe.description(description=summary, isSummary=True)
        ## FOR
    ## FOR
    
    atomfeed = fg.atom_str(pretty=True) # Get the ATOM feed as string
    fg.atom_file('pvldb-atom.xml') # Write the ATOM feed to a file
    
    rssfeed  = fg.rss_str(pretty=True) # Get the RSS feed as string
    fg.rss_file(RSS_FILE) # Write the RSS feed to a file
    
## DEF

## ==============================================
## postTwitter
## ==============================================
def postTwitter(args, paper):
    api = twitter.Api(consumer_key=args["twitter_consumer_key"],
                      consumer_secret=args["twitter_consumer_secret"],
                      access_token_key=args["twitter_access_token"],
                      access_token_secret=args["twitter_access_secret"])
## DEF


## ==============================================
## createDatabase
## ==============================================
def createDatabase():
    db = sqlite3.connect(DB_PATH)
    cur = db.cursor()
    
    sql = """
    CREATE TABLE papers (
        link VARCHAR(255) PRIMARY KEY,
        title TEXT NOT NULL,
        authors TEXT NOT NULL,
        volume INT NOT NULL,
        number INT NOT NULL,
        published DATE NOT NULL,
        twitter INT NOT NULL DEFAULT 0
    );"""
    cur.execute(sql)
    db.commit()
    db.close()
    
## FOR


## ==============================================
## main
## ==============================================
if __name__ == '__main__':
    aparser = argparse.ArgumentParser(description='PVLDB Announcements Script')
    aparser.add_argument('dbpath', help='Database Path')
    aparser.add_argument("--debug", action='store_true')

    ## RSS Parameters
    agroup = aparser.add_argument_group('RSS Parameters')
    agroup.add_argument('--rss', action='store_true', help='Genereate RSS/Atom file')
    agroup.add_argument('--rss-path', type=str, help='RSS output directory')

    ## Twitter Parameters
    agroup = aparser.add_argument_group('RSS Parameters')
    agroup.add_argument('--twitter', action='store_true', help='Post announcements on Twitter')
    agroup.add_argument('--twitter-consumer-key', type=str, help='Twitter Consumer Key')
    agroup.add_argument('--twitter-consumer-secret', type=str, help='Twitter Consumer Secret')
    agroup.add_argument('--twitter-access-token', type=str, help='Twitter Access Token Key')
    agroup.add_argument('--twitter-access-secret', type=str, help='Twitter Access Token Secret')
    
    args = vars(aparser.parse_args())

    ## ----------------------------------------------
    
    if args['debug']:
        LOG.setLevel(logging.DEBUG)

    
    # If they want to post to twitter, make sure they give us all the info
    # that we need to do this
    if args["twitter"]:
        LOG.debug("Checking twitter input arguments")
        for k in args.keys():
            if k.startswith("twitter") and args[k] is None:
                LOG.error("Missing '%s' input parameter for Twitter" % k)
                sys.exit(1)
        ## FOR
    ## IF

    ## ----------------------------------------------
    
    # Create the database if we don't have it
    if not os.path.exists(DB_PATH):
        createDatabase()
    db = sqlite3.connect(DB_PATH)
    cur = db.cursor()
        
    # Get the volume URLs
    volumes = getVolumeUrls(BASE_URL)
    papers = { }
    for v in volumes:
        try:
            p = getPapers(v)
            papers.update(p)
        except:
            logging.error("Unexpected error for " + v)
            raise
    #pprint(papers)
    
    # Always create the RSS files from scratch
    if args["rss"]:
        assert args["rss-path"]
        writeRSS(papers, args["rss-path"])
    
    # Figure out what papers are new
    new_papers = [ ]
    for d in reversed(sorted(papers.keys())):
        for p in papers[d]:
            sql = "SELECT * FROM papers WHERE link = ?"
            cur.execute(sql, (p["link"],))
            row = cur.fetchone()
            if row is None:
                print "Adding", p["link"]
                
                sql = """INSERT INTO papers (
                            link, title, authors, volume, number, published
                        ) VALUES (
                            ?, ?, ?, ?, ?, ?)"""
                cur.execute(sql, (p["link"], p["title"], p["authors"], p["volume"], p["number"], p["published"],))
                new_papers.append(p)
        ## FOR
    ## FOR
    db.commit()
    
    db.close()
    
## MAIN
    
    

